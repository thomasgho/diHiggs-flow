{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "from uproot_methods import TLorentzVectorArray as lv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T\n",
    "from pyro.nn import DenseNN, AutoRegressiveNN, ConditionalAutoRegressiveNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46559280\n"
     ]
    }
   ],
   "source": [
    "# df  = pd.read_parquet('/home/taymaz/Documents/Flows/Data/data.kinematics.BDT.2018.parquet', engine='pyarrow')\n",
    "# k = df[0:15000000]\n",
    "# del df\n",
    "\n",
    "df = pd.read_parquet('/home/taymaz/Documents/Flows/Data/data.kinematics.MDR_VEC.2018.parquet', engine='pyarrow', columns=['ntag', 'kinematic_region', 'pass_vbf_sel', 'm_h1', 'm_h2', 'log_pT_h1', 'log_pT_h2', 'eta_h1', 'eta_h2', 'log_dphi_hh'])\n",
    "print(len(df))\n",
    "k = df[0:5500000]\n",
    "del df\n",
    "\n",
    "df = pd.read_parquet('/home/taymaz/Documents/Flows/Data/test_pred_2b_MDR_VEC.parquet', engine='pyarrow')\n",
    "m = df[0:40000]\n",
    "del df\n",
    "\n",
    "#k = pd.read_hdf('/home/taymaz/Documents/Flows/Data/df_SM_2b_0.01_2b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# mask 2b/4b events & kinematic region & apply cuts\n",
    "mask_2b     = k['ntag'] == 2\n",
    "mask_4b     = k['ntag'] >= 4\n",
    "mask_CR     = k['kinematic_region'] == 2 \n",
    "mask_VR     = k['kinematic_region'] == 1 \n",
    "mask_SR     = k['kinematic_region'] == 0 \n",
    "mask_CRVR   = (k['kinematic_region'] == 1) | (k['kinematic_region'] == 2)\n",
    "mask_vbf    = k['pass_vbf_sel'] == False\n",
    "\n",
    "# train/test split\n",
    "split   = 0.5                       # train/test split ratio\n",
    "n       = int((1-split)*len(k))     # number of train samples\n",
    "\n",
    "# train/test data\n",
    "m_train = k.loc[(mask_2b & mask_CRVR & mask_vbf), ('m_h1', 'm_h2')][0:n]\n",
    "k_train = k.loc[(mask_2b & mask_CRVR & mask_vbf), ('log_pT_h1', 'log_pT_h2', 'eta_h1', 'eta_h2', 'log_dphi_hh')][0:n]\n",
    "#m_test  = k.loc[(mask_2b & mask_CRVR & mask_vbf), ('m_h1', 'm_h2')][n+1:len(k)]\n",
    "#k_test  = k.loc[(mask_2b & mask_CRVR & mask_vbf), ('log_pT_h1', 'log_pT_h2', 'eta_h1', 'eta_h2', 'log_dphi_hh')][n+1:len(k)]\n",
    "del k\n",
    "\n",
    "# normalize\n",
    "scaler_m_train = StandardScaler()\n",
    "m_train = torch.tensor(scaler_m_train.fit_transform(m_train)).float()\n",
    "scaler_k_train = StandardScaler()\n",
    "k_train = torch.tensor(scaler_k_train.fit_transform(k_train)).float()\n",
    "\n",
    "# mask SR in gaussian process data (for evaluation)\n",
    "def SR(m1,m2):\n",
    "    return np.sqrt(((m1-120)/(0.1*m1))**2+((m2-110)/(0.1*m2))**2)\n",
    "mask_SR_gp = SR(m['m_h1'], m['m_h2']) < 1.6\n",
    "m_eval = m.loc[(mask_SR_gp), ('m_h1', 'm_h2')]\n",
    "\n",
    "# normalize gaussian process data\n",
    "m_eval = torch.tensor(scaler_m_train.transform(m_eval)).float()\n",
    "del m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# wrap variables and conditionals into pytorch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# prepare dataloaders\n",
    "batch_size = 2**15\n",
    "trainset = Dataset(k_train, m_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
    "#testset = Dataset(k_test, m_test)\n",
    "#testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# flow model: see https://pyro.ai/examples/normalizing_flows_i.html\n",
    "# can choose any flow from: https://github.com/pyro-ppl/pyro/tree/dev/pyro/distributions/transforms\n",
    "\n",
    "# hyperparameters\n",
    "hypernet_type = 'spline_autoregressive'     # mlp, resnet or autoregressive\n",
    "num_flows     = 1                           # can stack multiple flows together\n",
    "dim_m         = 2       \n",
    "dim_k         = 5\n",
    "count_bins    = 10                          # spline bins\n",
    "bound         = 1.0                         # spline tail bound\n",
    "order         = 'linear'                    # spline tail order\n",
    "layers        = 5                           # number of hidden layers in mlp or residual block\n",
    "blocks        = 2                           # number of residual blocks\n",
    "hidden_dims_m = dim_m*10                    # for networks with input m\n",
    "hidden_dims_k = dim_k*10                    # for networks with input k\n",
    "dropout       = 0.1                         # dropout probability \n",
    "beta          = 1e-5                        # L2 regularization\n",
    "lr            = 5e-3                        # adam learning rate\n",
    "print_interval= 10                          # interval to print batch loss\n",
    "\n",
    "# base distributions chosen to be gaussian (can change to distribution of choice)\n",
    "dist_base_k = dist.Normal(torch.zeros(dim_k), torch.ones(dim_k), validate_args=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: 1, epoch: 0, [294912/449026 (64%)], train loss: 7.208\n",
      "bootstrap: 1, epoch: 1, [294912/449026 (64%)], train loss: 7.050\n",
      "bootstrap: 1, epoch: 2, [294912/449026 (64%)], train loss: 6.983\n",
      "bootstrap: 1, epoch: 3, [294912/449026 (64%)], train loss: 6.758\n",
      "bootstrap: 1, epoch: 4, [294912/449026 (64%)], train loss: 6.358\n",
      "bootstrap: 1, epoch: 5, [294912/449026 (64%)], train loss: 6.193\n",
      "bootstrap: 1, epoch: 6, [294912/449026 (64%)], train loss: 6.120\n",
      "bootstrap: 1, epoch: 7, [294912/449026 (64%)], train loss: 6.083\n",
      "bootstrap: 1, epoch: 8, [294912/449026 (64%)], train loss: 6.112\n",
      "bootstrap: 1, epoch: 9, [294912/449026 (64%)], train loss: 6.068\n",
      "bootstrap: 1, epoch: 10, [294912/449026 (64%)], train loss: 6.046\n",
      "bootstrap: 1, epoch: 11, [294912/449026 (64%)], train loss: 6.054\n",
      "bootstrap: 1, epoch: 12, [294912/449026 (64%)], train loss: 6.066\n",
      "bootstrap: 1, epoch: 13, [294912/449026 (64%)], train loss: 6.035\n",
      "bootstrap: 1, epoch: 14, [294912/449026 (64%)], train loss: 6.021\n",
      "bootstrap: 1, epoch: 15, [294912/449026 (64%)], train loss: 6.035\n",
      "bootstrap: 1, epoch: 16, [294912/449026 (64%)], train loss: 6.045\n",
      "bootstrap: 1, epoch: 17, [294912/449026 (64%)], train loss: 6.022\n",
      "bootstrap: 1, epoch: 18, [294912/449026 (64%)], train loss: 6.013\n",
      "bootstrap: 1, epoch: 19, [294912/449026 (64%)], train loss: 6.014\n",
      "bootstrap: 1, epoch: 20, [294912/449026 (64%)], train loss: 6.089\n",
      "bootstrap: 1, epoch: 21, [294912/449026 (64%)], train loss: 6.040\n",
      "bootstrap: 1, epoch: 22, [294912/449026 (64%)], train loss: 6.015\n",
      "bootstrap: 1, epoch: 23, [294912/449026 (64%)], train loss: 6.008\n",
      "bootstrap: 1, epoch: 24, [294912/449026 (64%)], train loss: 6.005\n",
      "bootstrap: 2, epoch: 0, [294912/449026 (64%)], train loss: 7.240\n",
      "bootstrap: 2, epoch: 1, [294912/449026 (64%)], train loss: 7.041\n",
      "bootstrap: 2, epoch: 2, [294912/449026 (64%)], train loss: 6.786\n",
      "bootstrap: 2, epoch: 3, [294912/449026 (64%)], train loss: 6.681\n",
      "bootstrap: 2, epoch: 4, [294912/449026 (64%)], train loss: 6.619\n",
      "bootstrap: 2, epoch: 5, [294912/449026 (64%)], train loss: 6.551\n",
      "bootstrap: 2, epoch: 6, [294912/449026 (64%)], train loss: 6.496\n",
      "bootstrap: 2, epoch: 7, [294912/449026 (64%)], train loss: 6.454\n",
      "bootstrap: 2, epoch: 8, [294912/449026 (64%)], train loss: 6.421\n",
      "bootstrap: 2, epoch: 9, [294912/449026 (64%)], train loss: 6.377\n",
      "bootstrap: 2, epoch: 10, [294912/449026 (64%)], train loss: 6.294\n",
      "bootstrap: 2, epoch: 11, [294912/449026 (64%)], train loss: 6.196\n",
      "bootstrap: 2, epoch: 12, [294912/449026 (64%)], train loss: 6.054\n",
      "bootstrap: 2, epoch: 13, [294912/449026 (64%)], train loss: 6.037\n",
      "bootstrap: 2, epoch: 14, [294912/449026 (64%)], train loss: 5.849\n",
      "bootstrap: 2, epoch: 15, [294912/449026 (64%)], train loss: 5.731\n",
      "bootstrap: 2, epoch: 16, [294912/449026 (64%)], train loss: 5.669\n",
      "bootstrap: 2, epoch: 17, [294912/449026 (64%)], train loss: 5.712\n",
      "bootstrap: 2, epoch: 18, [294912/449026 (64%)], train loss: 5.791\n",
      "bootstrap: 2, epoch: 19, [294912/449026 (64%)], train loss: 5.656\n",
      "bootstrap: 2, epoch: 20, [294912/449026 (64%)], train loss: 5.729\n",
      "bootstrap: 2, epoch: 21, [294912/449026 (64%)], train loss: 5.686\n",
      "bootstrap: 2, epoch: 22, [294912/449026 (64%)], train loss: 5.612\n",
      "bootstrap: 2, epoch: 23, [294912/449026 (64%)], train loss: 5.578\n",
      "bootstrap: 2, epoch: 24, [294912/449026 (64%)], train loss: 5.740\n",
      "bootstrap: 3, epoch: 0, [294912/449026 (64%)], train loss: 7.274\n",
      "bootstrap: 3, epoch: 1, [294912/449026 (64%)], train loss: 7.084\n",
      "bootstrap: 3, epoch: 2, [294912/449026 (64%)], train loss: 6.974\n",
      "bootstrap: 3, epoch: 3, [294912/449026 (64%)], train loss: 6.570\n",
      "bootstrap: 3, epoch: 4, [294912/449026 (64%)], train loss: 6.277\n",
      "bootstrap: 3, epoch: 5, [294912/449026 (64%)], train loss: 6.007\n",
      "bootstrap: 3, epoch: 6, [294912/449026 (64%)], train loss: 5.861\n",
      "bootstrap: 3, epoch: 7, [294912/449026 (64%)], train loss: 5.845\n",
      "bootstrap: 3, epoch: 8, [294912/449026 (64%)], train loss: 5.814\n",
      "bootstrap: 3, epoch: 9, [294912/449026 (64%)], train loss: 5.750\n",
      "bootstrap: 3, epoch: 10, [294912/449026 (64%)], train loss: 5.705\n",
      "bootstrap: 3, epoch: 11, [294912/449026 (64%)], train loss: 5.675\n",
      "bootstrap: 3, epoch: 12, [294912/449026 (64%)], train loss: 5.658\n",
      "bootstrap: 3, epoch: 13, [294912/449026 (64%)], train loss: 5.654\n",
      "bootstrap: 3, epoch: 14, [294912/449026 (64%)], train loss: 5.920\n",
      "bootstrap: 3, epoch: 15, [294912/449026 (64%)], train loss: 5.722\n",
      "bootstrap: 3, epoch: 16, [294912/449026 (64%)], train loss: 5.649\n",
      "bootstrap: 3, epoch: 17, [294912/449026 (64%)], train loss: 5.621\n",
      "bootstrap: 3, epoch: 18, [294912/449026 (64%)], train loss: 5.608\n",
      "bootstrap: 3, epoch: 19, [294912/449026 (64%)], train loss: 5.621\n",
      "bootstrap: 3, epoch: 20, [294912/449026 (64%)], train loss: 5.609\n",
      "bootstrap: 3, epoch: 21, [294912/449026 (64%)], train loss: 5.604\n",
      "bootstrap: 3, epoch: 22, [294912/449026 (64%)], train loss: 5.586\n",
      "bootstrap: 3, epoch: 23, [294912/449026 (64%)], train loss: 5.604\n",
      "bootstrap: 3, epoch: 24, [294912/449026 (64%)], train loss: 5.585\n",
      "bootstrap: 4, epoch: 0, [294912/449026 (64%)], train loss: 7.213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-28a5a8b5f880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# train log liklihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mln_p_k_given_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_k_given_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mln_p_k_given_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mevent_dim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_dim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             log_prob = log_prob - _sum_rightmost(transform.log_abs_det_jacobian(x, y),\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributions/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inv_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_abs_det_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributions/transforms.py\u001b[0m in \u001b[0;36m_inv_call\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0my_old\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_x_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pyro/distributions/transforms/spline_autoregressive.py\u001b[0m in \u001b[0;36m_inverse\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mspline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_log_detJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_log_detJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pyro/distributions/transforms/spline.py\u001b[0m in \u001b[0;36m_inverse\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0motherwise\u001b[0m \u001b[0mperforms\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minversion\u001b[0m \u001b[0mafresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \"\"\"\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_detJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspline_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_log_detJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_detJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pyro/distributions/transforms/spline.py\u001b[0m in \u001b[0;36mspline_op\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspline_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_detJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_monotonic_rational_spline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_detJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pyro/distributions/transforms/spline.py\u001b[0m in \u001b[0;36m_monotonic_rational_spline\u001b[0;34m(inputs, widths, heights, derivatives, lambdas, inverse, bound, min_bin_width, min_bin_height, min_derivative, min_lambda, eps)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minput_lambdas\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_lambdas\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwa\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mya\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 25         # total epochs\n",
    "bootstrap = 0      # initialise bootstrap\n",
    "bootstraps = 10    # number of re-trainings\n",
    "    \n",
    "while bootstrap < bootstraps:\n",
    "    bootstrap = bootstrap + 1\n",
    "        \n",
    "    # Neural Autoregressive Flows\n",
    "    if hypernet_type == 'affine_autoregressive':\n",
    "\n",
    "        hypernet_conditional = ConditionalAutoRegressiveNN(\n",
    "                                    input_dim=dim_k, \n",
    "                                    context_dim=dim_m, \n",
    "                                    hidden_dims=[hidden_dims_m]*layers, \n",
    "                                    skip_connections=False)\n",
    "\n",
    "        # kinematic autoregressive transform: input_dim, hypernet, count_bins\n",
    "        k_transform = [T.ConditionalAffineAutoregressive(hypernet_conditional) for _ in range(num_flows)]\n",
    "        dist_k_given_m = dist.ConditionalTransformedDistribution(dist_base_k, k_transform)\n",
    "    \n",
    "    # Autoregressive Spline Flows\n",
    "    elif hypernet_type == 'spline_autoregressive':\n",
    "\n",
    "        hypernet_conditional = ConditionalAutoRegressiveNN(\n",
    "                                    input_dim=dim_k, \n",
    "                                    context_dim=dim_m, \n",
    "                                    hidden_dims=[hidden_dims_m]*layers, \n",
    "                                    param_dims=[count_bins, count_bins, count_bins-1, count_bins] ,\n",
    "                                    skip_connections=False)\n",
    "\n",
    "        # kinematic autoregressive transform: input_dim, hypernet, count_bins\n",
    "        k_transform = [T.ConditionalSplineAutoregressive(dim_k, hypernet_conditional, count_bins=count_bins) for _ in range(num_flows)]\n",
    "        dist_k_given_m = dist.ConditionalTransformedDistribution(dist_base_k, k_transform)\n",
    "\n",
    "\n",
    "    modules   = torch.nn.ModuleList(k_transform).to(device)   \n",
    "    optimizer = torch.optim.Adam(modules.parameters(), lr=lr, weight_decay=beta)    \n",
    "    \n",
    "    for step in range(steps):\n",
    "        \n",
    "        running_train_loss = 0\n",
    "        for batch, (k_batch, m_batch) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # train log liklihood\n",
    "            ln_p_k_given_m = dist_k_given_m.condition(m_batch).log_prob(k_batch)        \n",
    "            loss_train = - ln_p_k_given_m.mean() \n",
    "\n",
    "            # take optimization step\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            dist_k_given_m.clear_cache()\n",
    "            running_train_loss += loss_train.item()\n",
    "            \n",
    "            if batch % print_interval == print_interval-1:\n",
    "                print('bootstrap: {}, epoch: {}, [{}/{} ({:.0f}%)], train loss: {:.3f}'.format(bootstrap, \n",
    "                                                                                       step, \n",
    "                                                                                       batch*batch_size, \n",
    "                                                                                       len(trainloader.dataset),\n",
    "                                                                                       100.*batch/len(trainloader),\n",
    "                                                                                       running_train_loss/print_interval))\n",
    "                running_train_loss = 0\n",
    "\n",
    "        # re-train if unstable\n",
    "        if np.isnan(running_train_loss):\n",
    "            bootstrap = bootstrap - 1\n",
    "            print('Unstable training. Restarting bootstrap...')\n",
    "            break\n",
    "    \n",
    "    # proceed to next bootstrap if stable   \n",
    "    if not np.isnan(running_train_loss):\n",
    "\n",
    "        # sample from transformed distribution\n",
    "        k_flow_ = dist_k_given_m.condition(m_train).sample(torch.Size([len(m_train),]))\n",
    "\n",
    "        # evaulate in SR\n",
    "        k_eval_ = dist_k_given_m.condition(m_eval).sample(torch.Size([len(m_eval),]))\n",
    "\n",
    "        # denormalize\n",
    "        k_train_ = scaler_k_train.inverse_transform(k_train.cpu().detach().numpy())\n",
    "        k_flow_  = scaler_k_train.inverse_transform(k_flow_.cpu().detach().numpy())\n",
    "        k_eval_  = scaler_k_train.inverse_transform(k_eval_.cpu().detach().numpy())\n",
    "        m_train_ = scaler_m_train.inverse_transform(m_train.cpu().detach().numpy())\n",
    "        m_eval_  = scaler_m_train.inverse_transform(m_eval.cpu().detach().numpy())\n",
    "        \n",
    "        # convert to pandas\n",
    "        k_train_ = pd.DataFrame(k_train_, columns = ['log_pT_h1', 'log_pT_h2', 'eta_h1', 'eta_h2', 'log_dphi_hh'])\n",
    "        k_flow_  = pd.DataFrame(k_flow_, columns = ['log_pT_h1', 'log_pT_h2', 'eta_h1', 'eta_h2', 'log_dphi_hh'])\n",
    "        k_eval_  = pd.DataFrame(k_eval_, columns = ['log_pT_h1', 'log_pT_h2', 'eta_h1', 'eta_h2', 'log_dphi_hh'])\n",
    "        m_train_ = pd.DataFrame(m_train_, columns = ['m_h1', 'm_h2'])\n",
    "        m_eval_  = pd.DataFrame(m_eval_, columns = ['m_h1', 'm_h2'])\n",
    "        \n",
    "        # undo preprocessing\n",
    "        for i in ('log_pT_h1', 'log_pT_h2', 'log_dphi_hh'):\n",
    "            k_flow_[i[4:]] = np.exp(k_flow_[i])\n",
    "            k_train_[i[4:]] = np.exp(k_train_[i])\n",
    "            k_eval_[i[4:]] = np.exp(k_eval_[i])\n",
    "        k_flow_['dphi_hh'] = np.pi - k_flow_['dphi_hh']\n",
    "        k_train_['dphi_hh'] = np.pi - k_train_['dphi_hh']\n",
    "        k_eval_['dphi_hh'] = np.pi - k_eval_['dphi_hh']\n",
    "\n",
    "        # find HC 4-vectors\n",
    "        hc1_pred = lv.from_ptetaphim(k_flow_['pT_h1'], k_flow_['eta_h1'], np.zeros_like(k_flow_.index).astype(float), m_train_['m_h1'])\n",
    "        hc2_pred = lv.from_ptetaphim(k_flow_['pT_h2'], k_flow_['eta_h2'], k_flow_['dphi_hh'], m_train_['m_h2'])\n",
    "        hc1_pred_SR = lv.from_ptetaphim(k_eval_['pT_h1'], k_eval_['eta_h1'], np.zeros_like(k_eval_.index).astype(float), m_eval_['m_h1'])\n",
    "        hc2_pred_SR = lv.from_ptetaphim(k_eval_['pT_h2'], k_eval_['eta_h2'], k_eval_['dphi_hh'], m_eval_['m_h2'])\n",
    "\n",
    "        # boost into the di-higgs rest frame to get cos(theta*)\n",
    "        hh_pred = hc1_pred + hc2_pred\n",
    "        hh_pred_SR = hc1_pred_SR + hc2_pred_SR\n",
    "        boost = - hh_pred.boostp3\n",
    "        boost_SR = - hh_pred_SR.boostp3\n",
    "        rest_hc1 = hc1_pred._to_cartesian().boost(boost)\n",
    "        rest_hc1_SR = hc1_pred_SR._to_cartesian().boost(boost_SR)\n",
    "\n",
    "        # calculate high level variables\n",
    "        k_flow_['m_hh'] = hh_pred.mass\n",
    "        k_flow_['m_hh_cor2'] = hh_pred.mass-hc1_pred.mass-hc2_pred.mass+250\n",
    "        k_flow_['absCosThetaStar'] = np.abs(np.cos(rest_hc1.theta))\n",
    "        k_flow_['pt_hh'] = hh_pred.pt\n",
    "        k_eval_['m_hh'] = hh_pred_SR.mass\n",
    "        k_eval_['m_hh_cor2'] = hh_pred_SR.mass-hc1_pred_SR.mass-hc2_pred_SR.mass+250\n",
    "        k_eval_['absCosThetaStar'] = np.abs(np.cos(rest_hc1_SR.theta))\n",
    "        k_eval_['pt_hh'] = hh_pred_SR.pt\n",
    "\n",
    "        # append conditional variables\n",
    "        k_flow_['m_h1'] = m_train_['m_h1']\n",
    "        k_flow_['m_h2'] = m_train_['m_h2']\n",
    "        k_eval_['m_h1'] = m_eval_['m_h1']\n",
    "        k_eval_['m_h2'] = m_eval_['m_h2'] \n",
    "\n",
    "        # save results\n",
    "        k_flow_.to_hdf('train_bootstrap_{}.h5'.format(bootstrap), key='df', mode='w')\n",
    "        k_eval_.to_hdf('eval_bootstrap_{}.h5'.format(bootstrap), key='df', mode='w')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
